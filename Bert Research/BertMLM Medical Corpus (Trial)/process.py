# -*- coding: utf-8 -*-
"""BertMLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TyXNd0W61UVgon0iYcDR0oLx6lQFasqi
"""

import os
#assert os.environ['COLAB_TPU_ADDR']

#VERSION = "1.5"  #@param ["1.5" , "20200325", "nightly"]
#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
#!python pytorch-xla-env-setup.py --version $VERSION

#!pip install transformers

#!kaggle datasets download -d allen-institute-for-ai/CORD-19-research-challenge

#!unzip -qq  CORD-19-research-challenge.zip

import re
import glob
import json
import pickle
import torch
import torch
import numpy as np
import transformers
import pandas as pd
from tqdm.auto import tqdm
import torch.nn as nn
from sklearn import model_selection
#import torch_xla.core.xla_model as xm
from transformers import AdamW, get_linear_schedule_with_warmup

root_path = "data"
metadata_path = "{}/metadata.csv".format(root_path)
meta_df = pd.read_csv(metadata_path, dtype={
    'pubmed_id': str,
    'Microsoft Academic Paper ID': str, 
    'doi': str
})
meta_df.head()

all_json = glob.glob("{}/**/*.json".format(root_path), recursive=True)

class FileReader:
    def __init__(self, file_path):
        with open(file_path) as file:
            content = json.load(file)
            self.paper_id = content['paper_id']
            self.abstract = []
            self.body_text = []
            # Abstract
            try:
                for entry in content['abstract']:
                    self.abstract.append(entry['text'])
            except:
                self.abstract.append("No abstract available")
            for entry in content["body_text"]:
                self.body_text.append(entry['text'])
            self.abstract = '. '.join(self.abstract)
            self.body_text = '. '.join(self.body_text)
    def __repr__(self):
        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'
first_row = FileReader(all_json[1])
print(first_row)

dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}
for idx, entry in tqdm(enumerate(all_json), total=len(all_json)):
    if idx % (len(all_json) // 10) == 0:
        print(f'Processing index: {idx} of {len(all_json)}')
    content = FileReader(entry)
    
    # get metadata information
    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]
    # no metadata, skip this paper
    if len(meta_data) == 0:
        continue
    
    dict_['paper_id'].append(content.paper_id)
    dict_['abstract'].append(content.abstract)
    dict_['body_text'].append(content.body_text)
    
    try:
        authors = meta_data['authors'].values[0].split(';')
        dict_['authors'].append(". ".join(authors))
    except Exception as e:
        # if Null value
        dict_['authors'].append(meta_data['authors'].values[0])
    
    # add the title information
    dict_['title'].append(meta_data['title'].values[0])
    
    # add the journal information
    dict_['journal'].append(meta_data['journal'].values[0])

df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal'])
df_covid.head()

df_covid.drop_duplicates(['title'], inplace=True)
df_covid.dropna(subset=['body_text'], inplace=True)
len(df_covid)

def preprocessing(text):
    # remove single characters repeated at least 3 times for spacing error (e.g. s u m m a r y)
    text = re.sub(r'(\w\s){3,}', ' ', text)
    # replace tags (e.g. [NUM1]) with whitespace
    text = re.sub(r'\[[\d\,\s]+\]\s', ' ',text)
    # correctly spacing the tokens
    text = re.sub(r' {2,}', ' ', text)
    text = re.sub(r'\.{2,}', '.', text)
    # return lowercase text
    return text.lower()

df_covid['body_text'] = df_covid['body_text'].apply(preprocessing)

covid_terms =['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov', '2019ncov', '2019 n cov', '2019n cov',
              'ncov 2019', 'n cov 2019', 'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',
              'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']
covid_terms = [preprocessing(elem) for elem in covid_terms]
covid_terms = re.compile('|'.join(covid_terms))

def checkCovid(text, covid_terms):
    return bool(covid_terms.search(text))

df_covid['is_covid'] = df_covid['body_text'].apply(checkCovid, covid_terms=covid_terms)

df_covid_only = df_covid[df_covid['is_covid']==True]
len(df_covid_only)

tokenizer = transformers.AutoTokenizer.from_pretrained("bert-large-uncased-whole-word-masking-finetuned-squad")

def firstFullStopIdx(token_list, token_stops):
    """
    Returns the index of first full-stop token appearing.  
    """
    idxs = []
    for stop in token_stops:
        if stop in token_list:
            idxs.append(token_list.index(stop))
    minIdx = min(idxs) if idxs else len(token_list)
    return minIdx

puncts = ['!', '.', '?', ';']
puncts_tokens = [tokenizer.tokenize(x)[0] for x in puncts]

def splitTokens(tokens, punct_tokens, split_length=90):
    """
    To avoid splitting a sentence and lose the semantic meaning of it, a paper is splitted into chunks in such
    a way that each chunk ends with a full-stop token (['.' ';' '?' or '!']) 
    """
    splitted_tokens = []
    n = int(len(tokens) / split_length)
    for _ in range(n):
        # to not have too long parapraphs, the nearest fullstop is searched both in the previous and the next strings.
        prev_stop_idx = firstFullStopIdx(tokens[:split_length][::-1], puncts_tokens)
        next_stop_idx = firstFullStopIdx(tokens[split_length:], puncts_tokens)
        if prev_stop_idx < next_stop_idx:
            splitted_tokens.append(tokens[:split_length - prev_stop_idx])
            tokens = tokens[split_length - prev_stop_idx:]
        else:
            splitted_tokens.append(tokens[:split_length + next_stop_idx + 1])
            tokens = tokens[split_length + next_stop_idx + 1:]        
    if len(tokens) != 0:
        splitted_tokens.append(tokens)
    return splitted_tokens

def splitParagraph(text, split_length=90):
    tokens = tokenizer.tokenize(text)
    splitted_tokens = splitTokens(tokens, puncts_tokens, split_length)
    return [tokenizer.convert_tokens_to_string(x) for x in splitted_tokens]

df_covid_only['body_text'] = df_covid_only['body_text'].apply(splitParagraph)

with open('df_covid_only_90tk_parags.pkl', 'wb') as output:
    pickle.dump(df_covid_only, output)
